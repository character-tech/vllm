FROM us-central1-docker.pkg.dev/character-ai/vllm/vllm-forked:d0ab76f as builder

RUN sudo apt install rsync -y

COPY . /mnt/vllm

# Print hash of source file before rsync
RUN sha256sum /mnt/vllm/vllm/entrypoints/openai/api_server.py

# Print hash of destination file before rsync (if it exists)
RUN VLLM_PATH=$(python3 -c "import vllm; print(vllm.__path__[0])") && \
    if [ -f "$VLLM_PATH/entrypoints/openai/api_server.py" ]; then \
        sha256sum $VLLM_PATH/entrypoints/openai/api_server.py; \
    else \
        echo "Destination file does not exist yet"; \
    fi

RUN VLLM_PATH=$(python3 -c "import vllm; print(vllm.__path__[0])") && \
    rsync -av --include="*/" --include="*.py" --exclude="*" /mnt/vllm/vllm/ $VLLM_PATH/

# Print hash of source file after rsync
RUN sha256sum /mnt/vllm/vllm/entrypoints/openai/api_server.py

# Print hash of destination file after rsync
RUN VLLM_PATH=$(python3 -c "import vllm; print(vllm.__path__[0])") && \
    sha256sum $VLLM_PATH/entrypoints/openai/api_server.py

RUN rm -rf /mnt/vllm
RUN python3 -c "import vllm; print('Custom vLLM loaded successfully')"

RUN python3 -m pip install --no-cache-dir --upgrade \
      opentelemetry-api==1.26.0 \
      opentelemetry-sdk==1.26.0 \
      opentelemetry-exporter-gcp-trace==1.7.0 \
      opentelemetry-exporter-otlp==1.26.0 \
      opentelemetry-semantic-conventions-ai==0.4.1 \
      google-cloud-trace==1.7.1

ENV HF_HOME=/huggingface/cache
